{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1: Classification Model Development\n"
      ],
      "metadata": {
        "id": "HCE7gA_rzDEh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code trains a BERT-based model to classify news text, focusing on reliable, accurate fake news detection."
      ],
      "metadata": {
        "id": "S-2wYUtJzsqd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports and Setup"
      ],
      "metadata": {
        "id": "fIQ-WLXQvxN2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary libraries\n",
        "!pip install transformers\n",
        "!pip install tweet-preprocessor\n",
        "!pip install textblob\n",
        "\n",
        "# Standard libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Sklearn libraries for preprocessing and evaluation\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "# Transformers library for BERT and tokenizer\n",
        "import transformers\n",
        "from transformers import AutoModel, BertTokenizerFast, AdamW\n",
        "\n",
        "# Tweet-preprocessor for text cleaning\n",
        "import preprocessor as p\n",
        "\n",
        "# PyTorch utilities for data handling\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# Textblob for Sentiment Analysis\n",
        "from textblob import TextBlob\n",
        "\n",
        "# Specify GPU if available, else default to CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Enable access to Google Drive for file storage\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8m2ybQ-ivl2b",
        "outputId": "1d04e153-ed4d-4386-9e21-62b724feb48e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: tweet-preprocessor in /usr/local/lib/python3.10/dist-packages (0.6.0)\n",
            "Requirement already satisfied: textblob in /usr/local/lib/python3.10/dist-packages (0.17.1)\n",
            "Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.10/dist-packages (from textblob) (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (4.66.5)\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Loading and Pre-Processing"
      ],
      "metadata": {
        "id": "dADya-p_v47Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code loads the training and validation datasets, applies preprocessing to clean up the text data (tweets), and encodes the labels (real/fake) into a numerical format for modeling."
      ],
      "metadata": {
        "id": "jzgNPgm1yVa3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to load data from TSV file into a DataFrame\n",
        "def getData(file):\n",
        "    \"\"\"Load TSV file and return DataFrame.\"\"\"\n",
        "    return pd.read_csv(file, delimiter=\"\\t\")\n",
        "\n",
        "# Define file paths for the training and validation datasets\n",
        "trainFilename = \"/content/drive/MyDrive/Fake News Detection Data/Constrain AI/Constraint_English_Train - Sheet1.tsv\"\n",
        "validFilename = \"/content/drive/MyDrive/Fake News Detection Data/Constrain AI/Constraint_English_Val - Sheet1.tsv\"\n",
        "\n",
        "# Load training and validation datasets\n",
        "trainDF = getData(trainFilename)\n",
        "validDF = getData(validFilename)\n",
        "print(\"Train Data Shape: \", trainDF.shape)\n",
        "print(\"Validation Data Shape: \", validDF.shape)\n",
        "\n",
        "# Function to preprocess tweets: removes special characters, converts to lowercase, removes hashtags and mentions\n",
        "def preprocessTweet(row):\n",
        "    text = row['tweet']\n",
        "    text = p.clean(text)\n",
        "    text = text.lower().replace(r'[^\\w\\s]', ' ').replace(r'\\s\\s+', ' ').replace(\"#\", \"\").replace(\"@\", \"\")\n",
        "    return text\n",
        "\n",
        "# Apply preprocessing function to each row in the dataset\n",
        "trainDF['processedTweet'] = trainDF.apply(preprocessTweet, axis=1)\n",
        "validDF['processedTweet'] = validDF.apply(preprocessTweet, axis=1)\n",
        "\n",
        "# Encode labels into numerical format for model compatibility\n",
        "labelEncoder = preprocessing.LabelEncoder()\n",
        "labelEncoder.fit(['real', 'fake'])\n",
        "trainDF['numericalLabels'] = labelEncoder.transform(trainDF['label'])\n",
        "validDF['numericalLabels'] = labelEncoder.transform(validDF['label'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9yCyVT5xwErK",
        "outputId": "8853058a-7012-4025-c0b5-120dd681d925"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Data Shape:  (6420, 3)\n",
            "Validation Data Shape:  (2140, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Getting Sentiment Scores, Adding Sentiment and Length to Dataset"
      ],
      "metadata": {
        "id": "xiC6vb97xUuT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to get sentiment score\n",
        "def get_sentiment(text):\n",
        "    polarity = TextBlob(text).sentiment.polarity\n",
        "    if polarity > 0:\n",
        "        return 1  # Positive\n",
        "    elif polarity == 0:\n",
        "        return 0  # Neutral\n",
        "    else:\n",
        "        return -1  # Negative"
      ],
      "metadata": {
        "id": "zTSTBTiPxXSp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply sentiment and length calculations\n",
        "trainDF['sentiment'] = trainDF['processedTweet'].apply(get_sentiment)\n",
        "trainDF['post_length'] = trainDF['processedTweet'].apply(lambda x: len(x.split()))\n",
        "\n",
        "validDF['sentiment'] = validDF['processedTweet'].apply(get_sentiment)\n",
        "validDF['post_length'] = validDF['processedTweet'].apply(lambda x: len(x.split()))"
      ],
      "metadata": {
        "id": "hMblLHzVxgj9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Splitting and Tokenization"
      ],
      "metadata": {
        "id": "FLKov04FwMKd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code splits the data into training and validation sets and uses BERT’s tokenizer to convert the text into a format suitable for model input (token IDs and attention masks)."
      ],
      "metadata": {
        "id": "GMt__LDGyRwX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModel, BertTokenizerFast\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "# Split training data for internal validation\n",
        "trainText, validText, trainLabels, validLabels = train_test_split(\n",
        "    trainDF['processedTweet'], trainDF['numericalLabels'],\n",
        "    random_state=2018,\n",
        "    test_size=0.04,\n",
        "    stratify=trainDF['numericalLabels']\n",
        ")\n",
        "\n",
        "# Recalculate sentiment and length based on the splits\n",
        "train_sentiment = torch.tensor(trainText.apply(get_sentiment).values).unsqueeze(1)\n",
        "train_length = torch.tensor(trainText.apply(lambda x: len(x.split())).values).unsqueeze(1)\n",
        "\n",
        "val_sentiment = torch.tensor(validText.apply(get_sentiment).values).unsqueeze(1)\n",
        "val_length = torch.tensor(validText.apply(lambda x: len(x.split())).values).unsqueeze(1)\n",
        "\n",
        "# Load pretrained BERT model and tokenizer\n",
        "bert = AutoModel.from_pretrained('bert-base-uncased')\n",
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Function to tokenize and encode sequences for input to BERT model\n",
        "def tokenize_text(text_data, tokenizer, max_length=200):\n",
        "    return tokenizer.batch_encode_plus(\n",
        "        text_data.tolist(),\n",
        "        max_length=max_length,\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "# Tokenize and encode train, validation, and test datasets\n",
        "tokens_train = tokenize_text(trainText, tokenizer)\n",
        "tokens_val = tokenize_text(validText, tokenizer)\n",
        "tokens_test = tokenize_text(validDF['processedTweet'], tokenizer)\n",
        "\n",
        "# Prepare tensors for model input\n",
        "# Training set\n",
        "train_seq, train_mask, train_y = tokens_train['input_ids'], tokens_train['attention_mask'], torch.tensor(trainLabels.tolist())\n",
        "\n",
        "# Validation set\n",
        "val_seq, val_mask, val_y = tokens_val['input_ids'], tokens_val['attention_mask'], torch.tensor(validLabels.tolist())\n",
        "\n",
        "# Test set (using validDF for sentiment and length)\n",
        "test_seq, test_mask, test_y = tokens_test['input_ids'], tokens_test['attention_mask'], torch.tensor(validDF['numericalLabels'].tolist())\n",
        "test_sentiment = torch.tensor(validDF['sentiment'].values).unsqueeze(1)\n",
        "test_length = torch.tensor(validDF['post_length'].values).unsqueeze(1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pBnporqcwRVk",
        "outputId": "d9007f4c-9f50-4952-93ff-6159b1e15304"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Loader Creation"
      ],
      "metadata": {
        "id": "ewTTt8U8wiWK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code cell creates DataLoader objects to efficiently batch and load data during training and validation, helping with faster processing"
      ],
      "metadata": {
        "id": "EeQBBs5zyaB6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define batch size\n",
        "batch_size = 32\n",
        "\n",
        "# Create DataLoader for train, validation, and test sets\n",
        "train_data = TensorDataset(train_seq, train_mask, train_sentiment, train_length, train_y)\n",
        "train_dataloader = DataLoader(train_data, sampler=RandomSampler(train_data), batch_size=batch_size)\n",
        "\n",
        "val_data = TensorDataset(val_seq, val_mask, val_sentiment, val_length, val_y)\n",
        "val_dataloader = DataLoader(val_data, sampler=SequentialSampler(val_data), batch_size=batch_size)\n",
        "\n",
        "test_data = TensorDataset(test_seq, test_mask, test_sentiment, test_length, test_y)\n",
        "test_dataloader = DataLoader(test_data, sampler=SequentialSampler(test_data), batch_size=batch_size)"
      ],
      "metadata": {
        "id": "t61yVYMDw35V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Definition"
      ],
      "metadata": {
        "id": "uZ4ZGpo6w46V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code sets up a custom BERT-based architecture, which uses BERT as a base model with additional fully connected layers for classification. BERT’s parameters are frozen to reduce training time."
      ],
      "metadata": {
        "id": "X-Y4WPSMyhPZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load pretrained BERT model\n",
        "bert = AutoModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Freeze BERT parameters to prevent updating during training\n",
        "for param in bert.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Define custom BERT model architecture with added layers\n",
        "class BERT_Arch(nn.Module):\n",
        "    def __init__(self, bert):\n",
        "        super(BERT_Arch, self).__init__()\n",
        "        self.bert = bert\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        # Adjust input size to account for BERT embeddings + 2 additional features (sentiment & length)\n",
        "        self.fc1 = nn.Linear(768 + 2, 512)  # 768 for BERT output, +2 for sentiment and post length\n",
        "        self.fc2 = nn.Linear(512, 2)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, sent_id, mask, sentiment, length):\n",
        "        # Pass input through BERT and get [CLS] token embedding\n",
        "        outputs = self.bert(sent_id, attention_mask=mask)\n",
        "        cls_hs = outputs.pooler_output\n",
        "\n",
        "        # Concatenate the BERT embedding with sentiment and post length\n",
        "        x = torch.cat((cls_hs, sentiment, length), dim=1)\n",
        "\n",
        "        # Pass concatenated features through fully connected layers\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return self.softmax(x)\n",
        "\n",
        "# Initialize model and send to device\n",
        "model = BERT_Arch(bert)\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "Mu3Qmb8PxAi3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optimizer, Loss Function, and Class Weights"
      ],
      "metadata": {
        "id": "yp22ogOYxLyK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code configures an AdamW optimizer and a cross-entropy loss function, adjusting for class imbalance by assigning weights to classes."
      ],
      "metadata": {
        "id": "rhbLlNjmyk7Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define optimizer\n",
        "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
        "\n",
        "# Compute class weights\n",
        "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(trainLabels), y=trainLabels)\n",
        "weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
        "cross_entropy = nn.NLLLoss(weight=weights)  # Weighted loss function"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z5EXSTkFxNG4",
        "outputId": "df8e30c1-f3d5-4c4c-a4fa-355428c0f6a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training and Evaluation Functions"
      ],
      "metadata": {
        "id": "cTepKkLQxSgQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code defines functions to handle model training and validation, calculating loss and updating the model’s parameters in each epoch."
      ],
      "metadata": {
        "id": "SSlOzD18yocT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define training function\n",
        "def train():\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in train_dataloader:\n",
        "        # Send each item in the batch to the device\n",
        "        sent_id, mask, sentiment, length, labels = [item.to(device) for item in batch]\n",
        "\n",
        "        # Zero out gradients\n",
        "        model.zero_grad()\n",
        "\n",
        "        # Forward pass, including sentiment and length as additional features\n",
        "        preds = model(sent_id, mask, sentiment, length)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = cross_entropy(preds, labels)\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Backward pass and optimization step\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "    return total_loss / len(train_dataloader)\n",
        "\n",
        "# Define evaluation function\n",
        "def evaluate():\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in val_dataloader:\n",
        "            # Send each item in the batch to the device\n",
        "            sent_id, mask, sentiment, length, labels = [item.to(device) for item in batch]\n",
        "\n",
        "            # Forward pass, including sentiment and length as additional features\n",
        "            preds = model(sent_id, mask, sentiment, length)\n",
        "\n",
        "            # Compute loss\n",
        "            loss = cross_entropy(preds, labels)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(val_dataloader)"
      ],
      "metadata": {
        "id": "MVceEnk7xToC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Loop"
      ],
      "metadata": {
        "id": "718IymXNxZBm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model is trained over multiple epochs, saving the model whenever validation loss improves, indicating better performance on unseen data."
      ],
      "metadata": {
        "id": "uUWYZHxnyrh3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving is based on best validation loss\n",
        "epochs = 15\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n",
        "    train_loss = train()\n",
        "    valid_loss = evaluate()\n",
        "\n",
        "    # Save model if validation loss improves\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), '/content/drive/MyDrive/Fake News Detection Data/Constrain AI/saved_weights.pt')\n",
        "\n",
        "    print(f\"Training Loss: {train_loss:.3f}\")\n",
        "    print(f\"Validation Loss: {valid_loss:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "flMtDEwhxcQ6",
        "outputId": "dc422e50-d2b7-4970-a57f-c86ffd085320"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/15\n",
            "Training Loss: 0.644\n",
            "Validation Loss: 0.572\n",
            "\n",
            "Epoch 2/15\n",
            "Training Loss: 0.599\n",
            "Validation Loss: 0.538\n",
            "\n",
            "Epoch 3/15\n",
            "Training Loss: 0.577\n",
            "Validation Loss: 0.519\n",
            "\n",
            "Epoch 4/15\n",
            "Training Loss: 0.556\n",
            "Validation Loss: 0.501\n",
            "\n",
            "Epoch 5/15\n",
            "Training Loss: 0.543\n",
            "Validation Loss: 0.484\n",
            "\n",
            "Epoch 6/15\n",
            "Training Loss: 0.526\n",
            "Validation Loss: 0.469\n",
            "\n",
            "Epoch 7/15\n",
            "Training Loss: 0.511\n",
            "Validation Loss: 0.456\n",
            "\n",
            "Epoch 8/15\n",
            "Training Loss: 0.498\n",
            "Validation Loss: 0.443\n",
            "\n",
            "Epoch 9/15\n",
            "Training Loss: 0.481\n",
            "Validation Loss: 0.432\n",
            "\n",
            "Epoch 10/15\n",
            "Training Loss: 0.468\n",
            "Validation Loss: 0.422\n",
            "\n",
            "Epoch 11/15\n",
            "Training Loss: 0.455\n",
            "Validation Loss: 0.412\n",
            "\n",
            "Epoch 12/15\n",
            "Training Loss: 0.446\n",
            "Validation Loss: 0.403\n",
            "\n",
            "Epoch 13/15\n",
            "Training Loss: 0.435\n",
            "Validation Loss: 0.397\n",
            "\n",
            "Epoch 14/15\n",
            "Training Loss: 0.424\n",
            "Validation Loss: 0.390\n",
            "\n",
            "Epoch 15/15\n",
            "Training Loss: 0.416\n",
            "Validation Loss: 0.383\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Testing and Evaluation"
      ],
      "metadata": {
        "id": "38WJ3zudxn-9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The best-performing model is loaded, and predictions are made on the test set. Finally, it generates a classification report showing the model’s performance metrics."
      ],
      "metadata": {
        "id": "8sVF4_vnyvgi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load best model\n",
        "model.load_state_dict(torch.load('/content/drive/MyDrive/Fake News Detection Data/Constrain AI/saved_weights.pt'))\n",
        "model.eval()\n",
        "\n",
        "# Perform predictions on test data\n",
        "preds = []\n",
        "with torch.no_grad():\n",
        "    for batch in test_dataloader:\n",
        "        # Unpack the batch to include sentiment and length\n",
        "        sent_id, mask, sentiment, length, labels = [item.to(device) for item in batch]\n",
        "\n",
        "        # Forward pass through the model, including sentiment and length\n",
        "        batch_preds = model(sent_id, mask, sentiment, length)\n",
        "\n",
        "        # Append predictions\n",
        "        preds.extend(batch_preds.detach().cpu().numpy())\n",
        "\n",
        "# Convert predictions to class labels\n",
        "preds = np.argmax(np.array(preds), axis=1)\n",
        "print(classification_report(test_y, preds))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g5-Y1dd_xqM1",
        "outputId": "be9f66f5-da18-4f07-d77d-3ac09e35a9aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-16-ee91211edcf0>:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load('/content/drive/MyDrive/Fake News Detection Data/Constrain AI/saved_weights.pt'))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.86      0.83      1020\n",
            "           1       0.86      0.79      0.83      1120\n",
            "\n",
            "    accuracy                           0.83      2140\n",
            "   macro avg       0.83      0.83      0.83      2140\n",
            "weighted avg       0.83      0.83      0.83      2140\n",
            "\n"
          ]
        }
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1: Classification Model Development\n"
      ],
      "metadata": {
        "id": "HCE7gA_rzDEh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code trains a BERT-based model to classify news text, focusing on reliable, accurate fake news detection."
      ],
      "metadata": {
        "id": "S-2wYUtJzsqd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports and Setup"
      ],
      "metadata": {
        "id": "fIQ-WLXQvxN2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary libraries\n",
        "!pip install transformers\n",
        "!pip install tweet-preprocessor\n",
        "!pip install textblob\n",
        "\n",
        "# Standard libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Sklearn libraries for preprocessing and evaluation\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "# Transformers library for BERT and tokenizer\n",
        "import transformers\n",
        "from transformers import AutoModel, BertTokenizerFast, AdamW\n",
        "\n",
        "# Tweet-preprocessor for text cleaning\n",
        "import preprocessor as p\n",
        "\n",
        "# PyTorch utilities for data handling\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# Textblob for Sentiment Analysis\n",
        "from textblob import TextBlob\n",
        "\n",
        "# Specify GPU if available, else default to CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Enable access to Google Drive for file storage\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8m2ybQ-ivl2b",
        "outputId": "1d04e153-ed4d-4386-9e21-62b724feb48e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: tweet-preprocessor in /usr/local/lib/python3.10/dist-packages (0.6.0)\n",
            "Requirement already satisfied: textblob in /usr/local/lib/python3.10/dist-packages (0.17.1)\n",
            "Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.10/dist-packages (from textblob) (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (4.66.5)\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Loading and Pre-Processing"
      ],
      "metadata": {
        "id": "dADya-p_v47Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This coad loads the training and validation datasets, applies preprocessing to clean up the text data (tweets), and encodes the labels (real/fake) into a numerical format for modeling."
      ],
      "metadata": {
        "id": "jzgNPgm1yVa3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to load data from TSV file into a DataFrame\n",
        "def getData(file):\n",
        "    \"\"\"Load TSV file and return DataFrame.\"\"\"\n",
        "    return pd.read_csv(file, delimiter=\"\\t\")\n",
        "\n",
        "# Define file paths for the training and validation datasets\n",
        "trainFilename = \"/content/drive/MyDrive/Fake News Detection Data/Constrain AI/Constraint_English_Train - Sheet1.tsv\"\n",
        "validFilename = \"/content/drive/MyDrive/Fake News Detection Data/Constrain AI/Constraint_English_Val - Sheet1.tsv\"\n",
        "\n",
        "# Load training and validation datasets\n",
        "trainDF = getData(trainFilename)\n",
        "validDF = getData(validFilename)\n",
        "print(\"Train Data Shape: \", trainDF.shape)\n",
        "print(\"Validation Data Shape: \", validDF.shape)\n",
        "\n",
        "# Function to preprocess tweets: removes special characters, converts to lowercase, removes hashtags and mentions\n",
        "def preprocessTweet(row):\n",
        "    text = row['tweet']\n",
        "    text = p.clean(text)\n",
        "    text = text.lower().replace(r'[^\\w\\s]', ' ').replace(r'\\s\\s+', ' ').replace(\"#\", \"\").replace(\"@\", \"\")\n",
        "    return text\n",
        "\n",
        "# Apply preprocessing function to each row in the dataset\n",
        "trainDF['processedTweet'] = trainDF.apply(preprocessTweet, axis=1)\n",
        "validDF['processedTweet'] = validDF.apply(preprocessTweet, axis=1)\n",
        "\n",
        "# Encode labels into numerical format for model compatibility\n",
        "labelEncoder = preprocessing.LabelEncoder()\n",
        "labelEncoder.fit(['real', 'fake'])\n",
        "trainDF['numericalLabels'] = labelEncoder.transform(trainDF['label'])\n",
        "validDF['numericalLabels'] = labelEncoder.transform(validDF['label'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9yCyVT5xwErK",
        "outputId": "8853058a-7012-4025-c0b5-120dd681d925"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Data Shape:  (6420, 3)\n",
            "Validation Data Shape:  (2140, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Getting Sentiment Score"
      ],
      "metadata": {
        "id": "xiC6vb97xUuT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to get sentiment score\n",
        "def get_sentiment(text):\n",
        "    polarity = TextBlob(text).sentiment.polarity\n",
        "    if polarity > 0:\n",
        "        return 1  # Positive\n",
        "    elif polarity == 0:\n",
        "        return 0  # Neutral\n",
        "    else:\n",
        "        return -1  # Negative"
      ],
      "metadata": {
        "id": "zTSTBTiPxXSp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sentiment and length to data set"
      ],
      "metadata": {
        "id": "w0eh1Y8Dxdga"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply sentiment and length calculations\n",
        "trainDF['sentiment'] = trainDF['processedTweet'].apply(get_sentiment)\n",
        "trainDF['post_length'] = trainDF['processedTweet'].apply(lambda x: len(x.split()))\n",
        "\n",
        "validDF['sentiment'] = validDF['processedTweet'].apply(get_sentiment)\n",
        "validDF['post_length'] = validDF['processedTweet'].apply(lambda x: len(x.split()))"
      ],
      "metadata": {
        "id": "hMblLHzVxgj9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Splitting and Tokenization"
      ],
      "metadata": {
        "id": "FLKov04FwMKd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code splits the data into training and validation sets and uses BERT’s tokenizer to convert the text into a format suitable for model input (token IDs and attention masks)."
      ],
      "metadata": {
        "id": "GMt__LDGyRwX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModel, BertTokenizerFast\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "# Split training data for internal validation\n",
        "trainText, validText, trainLabels, validLabels = train_test_split(\n",
        "    trainDF['processedTweet'], trainDF['numericalLabels'],\n",
        "    random_state=2018,\n",
        "    test_size=0.04,\n",
        "    stratify=trainDF['numericalLabels']\n",
        ")\n",
        "\n",
        "# Recalculate sentiment and length based on the splits\n",
        "train_sentiment = torch.tensor(trainText.apply(get_sentiment).values).unsqueeze(1)\n",
        "train_length = torch.tensor(trainText.apply(lambda x: len(x.split())).values).unsqueeze(1)\n",
        "\n",
        "val_sentiment = torch.tensor(validText.apply(get_sentiment).values).unsqueeze(1)\n",
        "val_length = torch.tensor(validText.apply(lambda x: len(x.split())).values).unsqueeze(1)\n",
        "\n",
        "# Load pretrained BERT model and tokenizer\n",
        "bert = AutoModel.from_pretrained('bert-base-uncased')\n",
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Function to tokenize and encode sequences for input to BERT model\n",
        "def tokenize_text(text_data, tokenizer, max_length=200):\n",
        "    return tokenizer.batch_encode_plus(\n",
        "        text_data.tolist(),\n",
        "        max_length=max_length,\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "# Tokenize and encode train, validation, and test datasets\n",
        "tokens_train = tokenize_text(trainText, tokenizer)\n",
        "tokens_val = tokenize_text(validText, tokenizer)\n",
        "tokens_test = tokenize_text(validDF['processedTweet'], tokenizer)\n",
        "\n",
        "# Prepare tensors for model input\n",
        "# Training set\n",
        "train_seq, train_mask, train_y = tokens_train['input_ids'], tokens_train['attention_mask'], torch.tensor(trainLabels.tolist())\n",
        "\n",
        "# Validation set\n",
        "val_seq, val_mask, val_y = tokens_val['input_ids'], tokens_val['attention_mask'], torch.tensor(validLabels.tolist())\n",
        "\n",
        "# Test set (using validDF for sentiment and length)\n",
        "test_seq, test_mask, test_y = tokens_test['input_ids'], tokens_test['attention_mask'], torch.tensor(validDF['numericalLabels'].tolist())\n",
        "test_sentiment = torch.tensor(validDF['sentiment'].values).unsqueeze(1)\n",
        "test_length = torch.tensor(validDF['post_length'].values).unsqueeze(1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pBnporqcwRVk",
        "outputId": "d9007f4c-9f50-4952-93ff-6159b1e15304"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Loader Creation"
      ],
      "metadata": {
        "id": "ewTTt8U8wiWK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code cell creates DataLoader objects to efficiently batch and load data during training and validation, helping with faster processing"
      ],
      "metadata": {
        "id": "EeQBBs5zyaB6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define batch size\n",
        "batch_size = 32\n",
        "\n",
        "# Create DataLoader for train, validation, and test sets\n",
        "train_data = TensorDataset(train_seq, train_mask, train_sentiment, train_length, train_y)\n",
        "train_dataloader = DataLoader(train_data, sampler=RandomSampler(train_data), batch_size=batch_size)\n",
        "\n",
        "val_data = TensorDataset(val_seq, val_mask, val_sentiment, val_length, val_y)\n",
        "val_dataloader = DataLoader(val_data, sampler=SequentialSampler(val_data), batch_size=batch_size)\n",
        "\n",
        "test_data = TensorDataset(test_seq, test_mask, test_sentiment, test_length, test_y)\n",
        "test_dataloader = DataLoader(test_data, sampler=SequentialSampler(test_data), batch_size=batch_size)"
      ],
      "metadata": {
        "id": "t61yVYMDw35V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Definition"
      ],
      "metadata": {
        "id": "uZ4ZGpo6w46V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code sets up a custom BERT-based architecture, which uses BERT as a base model with additional fully connected layers for classification. BERT’s parameters are frozen to reduce training time."
      ],
      "metadata": {
        "id": "X-Y4WPSMyhPZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load pretrained BERT model\n",
        "bert = AutoModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Freeze BERT parameters to prevent updating during training\n",
        "for param in bert.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Define custom BERT model architecture with added layers\n",
        "class BERT_Arch(nn.Module):\n",
        "    def __init__(self, bert):\n",
        "        super(BERT_Arch, self).__init__()\n",
        "        self.bert = bert\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        # Adjust input size to account for BERT embeddings + 2 additional features (sentiment & length)\n",
        "        self.fc1 = nn.Linear(768 + 2, 512)  # 768 for BERT output, +2 for sentiment and post length\n",
        "        self.fc2 = nn.Linear(512, 2)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, sent_id, mask, sentiment, length):\n",
        "        # Pass input through BERT and get [CLS] token embedding\n",
        "        outputs = self.bert(sent_id, attention_mask=mask)\n",
        "        cls_hs = outputs.pooler_output\n",
        "\n",
        "        # Concatenate the BERT embedding with sentiment and post length\n",
        "        x = torch.cat((cls_hs, sentiment, length), dim=1)\n",
        "\n",
        "        # Pass concatenated features through fully connected layers\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return self.softmax(x)\n",
        "\n",
        "# Initialize model and send to device\n",
        "model = BERT_Arch(bert)\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "Mu3Qmb8PxAi3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optimizer, Loss Function, and Class Weights"
      ],
      "metadata": {
        "id": "yp22ogOYxLyK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code configures an AdamW optimizer and a cross-entropy loss function, adjusting for class imbalance by assigning weights to classes."
      ],
      "metadata": {
        "id": "rhbLlNjmyk7Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define optimizer\n",
        "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
        "\n",
        "# Compute class weights\n",
        "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(trainLabels), y=trainLabels)\n",
        "weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
        "cross_entropy = nn.NLLLoss(weight=weights)  # Weighted loss function"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z5EXSTkFxNG4",
        "outputId": "df8e30c1-f3d5-4c4c-a4fa-355428c0f6a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training and Evaluation Functions"
      ],
      "metadata": {
        "id": "cTepKkLQxSgQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code defines functions to handle model training and validation, calculating loss and updating the model’s parameters in each epoch."
      ],
      "metadata": {
        "id": "SSlOzD18yocT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define training function\n",
        "def train():\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in train_dataloader:\n",
        "        # Send each item in the batch to the device\n",
        "        sent_id, mask, sentiment, length, labels = [item.to(device) for item in batch]\n",
        "\n",
        "        # Zero out gradients\n",
        "        model.zero_grad()\n",
        "\n",
        "        # Forward pass, including sentiment and length as additional features\n",
        "        preds = model(sent_id, mask, sentiment, length)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = cross_entropy(preds, labels)\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Backward pass and optimization step\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "    return total_loss / len(train_dataloader)\n",
        "\n",
        "# Define evaluation function\n",
        "def evaluate():\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in val_dataloader:\n",
        "            # Send each item in the batch to the device\n",
        "            sent_id, mask, sentiment, length, labels = [item.to(device) for item in batch]\n",
        "\n",
        "            # Forward pass, including sentiment and length as additional features\n",
        "            preds = model(sent_id, mask, sentiment, length)\n",
        "\n",
        "            # Compute loss\n",
        "            loss = cross_entropy(preds, labels)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(val_dataloader)"
      ],
      "metadata": {
        "id": "MVceEnk7xToC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Loop"
      ],
      "metadata": {
        "id": "718IymXNxZBm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model is trained over multiple epochs, saving the model whenever validation loss improves, indicating better performance on unseen data."
      ],
      "metadata": {
        "id": "uUWYZHxnyrh3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving is based on best validation loss\n",
        "epochs = 15\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n",
        "    train_loss = train()\n",
        "    valid_loss = evaluate()\n",
        "\n",
        "    # Save model if validation loss improves\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), '/content/drive/MyDrive/Fake News Detection Data/Constrain AI/saved_weights.pt')\n",
        "\n",
        "    print(f\"Training Loss: {train_loss:.3f}\")\n",
        "    print(f\"Validation Loss: {valid_loss:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "flMtDEwhxcQ6",
        "outputId": "dc422e50-d2b7-4970-a57f-c86ffd085320"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/15\n",
            "Training Loss: 0.644\n",
            "Validation Loss: 0.572\n",
            "\n",
            "Epoch 2/15\n",
            "Training Loss: 0.599\n",
            "Validation Loss: 0.538\n",
            "\n",
            "Epoch 3/15\n",
            "Training Loss: 0.577\n",
            "Validation Loss: 0.519\n",
            "\n",
            "Epoch 4/15\n",
            "Training Loss: 0.556\n",
            "Validation Loss: 0.501\n",
            "\n",
            "Epoch 5/15\n",
            "Training Loss: 0.543\n",
            "Validation Loss: 0.484\n",
            "\n",
            "Epoch 6/15\n",
            "Training Loss: 0.526\n",
            "Validation Loss: 0.469\n",
            "\n",
            "Epoch 7/15\n",
            "Training Loss: 0.511\n",
            "Validation Loss: 0.456\n",
            "\n",
            "Epoch 8/15\n",
            "Training Loss: 0.498\n",
            "Validation Loss: 0.443\n",
            "\n",
            "Epoch 9/15\n",
            "Training Loss: 0.481\n",
            "Validation Loss: 0.432\n",
            "\n",
            "Epoch 10/15\n",
            "Training Loss: 0.468\n",
            "Validation Loss: 0.422\n",
            "\n",
            "Epoch 11/15\n",
            "Training Loss: 0.455\n",
            "Validation Loss: 0.412\n",
            "\n",
            "Epoch 12/15\n",
            "Training Loss: 0.446\n",
            "Validation Loss: 0.403\n",
            "\n",
            "Epoch 13/15\n",
            "Training Loss: 0.435\n",
            "Validation Loss: 0.397\n",
            "\n",
            "Epoch 14/15\n",
            "Training Loss: 0.424\n",
            "Validation Loss: 0.390\n",
            "\n",
            "Epoch 15/15\n",
            "Training Loss: 0.416\n",
            "Validation Loss: 0.383\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Testing and Evaluation"
      ],
      "metadata": {
        "id": "38WJ3zudxn-9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The best-performing model is loaded, and predictions are made on the test set. Finally, it generates a classification report showing the model’s performance metrics."
      ],
      "metadata": {
        "id": "8sVF4_vnyvgi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load best model\n",
        "model.load_state_dict(torch.load('/content/drive/MyDrive/Fake News Detection Data/Constrain AI/saved_weights.pt'))\n",
        "model.eval()\n",
        "\n",
        "# Perform predictions on test data\n",
        "preds = []\n",
        "with torch.no_grad():\n",
        "    for batch in test_dataloader:\n",
        "        # Unpack the batch to include sentiment and length\n",
        "        sent_id, mask, sentiment, length, labels = [item.to(device) for item in batch]\n",
        "\n",
        "        # Forward pass through the model, including sentiment and length\n",
        "        batch_preds = model(sent_id, mask, sentiment, length)\n",
        "\n",
        "        # Append predictions\n",
        "        preds.extend(batch_preds.detach().cpu().numpy())\n",
        "\n",
        "# Convert predictions to class labels\n",
        "preds = np.argmax(np.array(preds), axis=1)\n",
        "print(classification_report(test_y, preds))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g5-Y1dd_xqM1",
        "outputId": "be9f66f5-da18-4f07-d77d-3ac09e35a9aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-16-ee91211edcf0>:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load('/content/drive/MyDrive/Fake News Detection Data/Constrain AI/saved_weights.pt'))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.86      0.83      1020\n",
            "           1       0.86      0.79      0.83      1120\n",
            "\n",
            "    accuracy                           0.83      2140\n",
            "   macro avg       0.83      0.83      0.83      2140\n",
            "weighted avg       0.83      0.83      0.83      2140\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Given your paper's focus on identifying features that are relevant to distinguishing fake and real news, whether by a model or a human, including the ablation study could add significant value. Here’s why:\n",
        "\n",
        "1. **Feature Relevance in Model Prediction**: The ablation study can demonstrate whether features like sentiment and length are practically useful in a complex model like BERT, which can guide future work in model feature selection and help readers understand which features truly contribute to prediction accuracy.\n",
        "\n",
        "2. **Human Interpretability**: Since the paper also considers features that humans might use, the statistical significance of sentiment and length is highly relevant. Including the ablation study helps emphasize the difference between features that are *statistically relevant* and those that *actually improve model predictions*, a distinction that might be insightful for both modelers and human evaluators.\n",
        "\n",
        "3. **Balanced View of Feature Impact**: By presenting both statistical tests and ablation results, you show a balanced view: some features (like sentiment and length) may be important for human interpretation or basic statistical distinctions, but their utility may not carry over directly to model performance. This dual perspective strengthens the paper by addressing both human interpretability and model efficacy.\n",
        "\n",
        "### Suggested Integration\n",
        "\n",
        "You could structure this section as follows:\n",
        "\n",
        "1. **Statistical Relevance**: Present hypothesis testing results to show that sentiment and length are statistically significant in distinguishing fake from real news.\n",
        "2. **Ablation Study**: Show that while these features are statistically relevant, they don’t notably improve model performance, suggesting that BERT embeddings may already capture this information.\n",
        "3. **Implications for Human and Model Distinction**: Discuss how humans might rely on sentiment or length cues when distinguishing news types, whereas a model like BERT doesn’t necessarily benefit from explicit inclusion of these features.\n",
        "\n",
        "This approach would provide a comprehensive answer to your research question, showcasing the distinctions and overlaps between features useful to humans and to machine models."
      ],
      "metadata": {
        "id": "zHkzl1gmKXA7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If your research paper shows that certain features (like sentiment and length) are statistically significant in distinguishing real from fake news but including them in the model didn’t improve performance—and, in fact, slightly decreased it—this outcome is still valuable. Here’s how to interpret and present these findings effectively:\n",
        "\n",
        "### 1. **Highlight the Statistical Significance as an Insight into the Dataset**\n",
        "\n",
        "   - **Interpretation of Statistical Findings**: You can emphasize that features like sentiment and length **are meaningfully associated with news type** (real vs. fake), as shown by hypothesis testing. This is important information for understanding the dataset and the types of language patterns that may characterize fake news.\n",
        "   - **Feature Relevance**: Acknowledge that these features have statistical relevance, which means they differentiate fake news from real news in general terms, even if they don’t improve the model’s performance in a predictive setting.\n",
        "\n",
        "### 2. **Explain Why BERT’s Embeddings May Have Diminished the Need for These Features**\n",
        "\n",
        "   - **Redundant Information**: Mention that BERT embeddings already capture complex language representations, including sentiment and length implicitly. Because BERT has been pre-trained on large amounts of text data, it likely already encodes sentiment-related patterns and length nuances within its embeddings.\n",
        "   - **Complex Model Dynamics**: Explain that adding explicit sentiment and length features might have created redundancy, leading to a slight decrease in performance. This could be because the model is now handling duplicate information or is overfitting on sentiment and length, which are already well-represented in BERT’s embeddings.\n",
        "\n",
        "### 3. **Discuss Implications for Future Research and Practical Applications**\n",
        "\n",
        "   - **Importance of Hypothesis Testing**: Emphasize that statistical tests remain essential for understanding data characteristics, even if these features don’t add predictive power to complex models. In real-world applications, knowing that sentiment and length are distinguishing factors can guide content analysis, policy-making, or simpler models where these features may matter more.\n",
        "   - **Feature Engineering Recommendations**: Suggest that researchers using simpler models, such as logistic regression or support vector machines, might find these features more useful. Additionally, sentiment and length could be more impactful for specific sub-types of fake news or different domains (e.g., health vs. political news).\n",
        "   - **Future Model Interpretability**: Encourage future work to explore alternative interpretability techniques, like attention weights in Transformer models, to understand more about how BERT utilizes language patterns in fake news detection.\n",
        "\n",
        "### 4. **Conclude with the Value of Both Statistical and Predictive Analysis**\n",
        "\n",
        "   - **Dual Analysis Approach**: Reinforce that combining statistical tests with predictive modeling provided deeper insights into the dataset and the model’s behavior. Although the features did not boost performance, the statistical significance reveals meaningful distinctions in the data.\n",
        "   - **Balanced Conclusion**: Summarize that while sentiment and length are significant in distinguishing real from fake news, they are not necessarily beneficial in complex models like BERT that already capture nuanced language features. This insight can help refine feature engineering practices in NLP research.\n",
        "\n",
        "### Summary of Key Points for Your Research Paper\n",
        "\n",
        "1. **Statistical Significance**: Sentiment and length are statistically significant in distinguishing fake news from real news.\n",
        "2. **Model Performance Impact**: Including these features didn’t improve BERT’s performance, likely due to redundancy.\n",
        "3. **Future Directions**: Highlight potential applications in simpler models or as insights for content analysis.\n",
        "4. **Value of Hypothesis Testing**: Statistical testing remains crucial for understanding dataset characteristics, even if the features don’t directly benefit complex models.\n",
        "\n",
        "This analysis strengthens your paper by showing a comprehensive approach to feature evaluation, balancing statistical significance with practical implications in model performance."
      ],
      "metadata": {
        "id": "c2zbWEb-KbVs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Given your research focus and the analyses you’ve conducted, here are some potential research questions that align with your investigation of feature relevance for distinguishing fake and real news:\n",
        "\n",
        "1. **Primary Research Question**:\n",
        "   - *\"What features are relevant for distinguishing fake news from real news, and how do these features contribute to classification when analyzed statistically versus when included in a predictive model?\"*\n",
        "   - This question captures your goal of understanding feature relevance both from a statistical perspective (for human interpretability) and in a practical, predictive sense (for model performance).\n",
        "\n",
        "2. **Supporting Research Questions**:\n",
        "   - *\"Do features like sentiment and length provide statistically significant distinctions between fake and real news?\"*\n",
        "     - This question addresses the role of hypothesis testing in identifying meaningful differences between fake and real news based on linguistic features.\n",
        "   - *\"Does including statistically relevant features, such as sentiment and post length, improve the performance of a complex model like BERT in classifying fake and real news?\"*\n",
        "     - This question explores the practical utility of adding these features to a model that already captures language nuances, providing insight into the value of explicit versus implicit feature use in machine learning.\n",
        "   - *\"How do features that are relevant to humans for identifying fake news compare to those that are useful for a machine learning model?\"*\n",
        "     - This emphasizes the distinction between human and model interpretability, addressing whether features like sentiment and length are useful in similar ways for humans and machines.\n",
        "\n",
        "### Summary\n",
        "\n",
        "Your primary question delves into the relevance of features in both statistical and predictive contexts, while the supporting questions help dissect the nuances of feature significance for humans versus models. This setup highlights a dual approach, analyzing how and why features matter from different perspectives."
      ],
      "metadata": {
        "id": "LzROthCWKcGd"
      }
    }
  ]
}